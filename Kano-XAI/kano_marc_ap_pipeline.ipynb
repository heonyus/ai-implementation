{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf28696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U datasets pandas numpy scikit-learn gensim nltk shap joblib tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fede4f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, math, warnings, joblib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 임베딩/군집\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# 감성\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 모델링/평가\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def ensure_nltk():\n",
    "    \"\"\"NLTK 리소스 자동 다운로드: VADER, stopwords\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon')\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    \"\"\"공백 정리, 기본 클리닝\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(t)).strip()\n",
    "\n",
    "\n",
    "def tokenize_en(t: str) -> List[str]:\n",
    "    \"\"\"간단 영어 토큰화: 알파벳 토큰 + 불용어 제거\"\"\"\n",
    "    toks = [w.lower() for w in wordpunct_tokenize(t)]\n",
    "    toks = [w for w in toks if any(c.isalpha() for c in w)]\n",
    "    sw = set(stopwords.words('english'))\n",
    "    toks = [w for w in toks if w not in sw and len(w) > 1]\n",
    "    return toks\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    w2v_size: int = 100\n",
    "    ap_affinity: str = \"precomputed\"  # \"precomputed\" or \"euclidean\"\n",
    "    ap_damping: float = 0.7\n",
    "    ap_preference: str = \"median\"     # \"median\" / \"min\" / \"none\"\n",
    "    random_state: int = 42\n",
    "    max_train: int = 50000            # 리소스 절약용 샘플 수 (0이면 전체)\n",
    "    max_test: int = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558703a",
   "metadata": {},
   "source": [
    "## 2) 이론 요약 (필요 공식만 간단히)\n",
    "\n",
    "### 2.1 Affinity Propagation (AP) 핵심 개념\n",
    "- 유사도 행렬 $S \\in \\mathbb{R}^{N \\times N}$ 사용함 (본 노트북: 코사인 유사도)\n",
    "- 두 가지 메시지 업데이트함: **책임(responsibility)** $r(i,k)$, **가용성(availability)** $a(i,k)$\n",
    "- 대표 업데이트 식(직관만 표시):  \n",
    "$ r(i,k) \\leftarrow S(i,k) - \\max_{k' \\neq k} \\{ a(i,k') + S(i,k') \\}$  \n",
    "$ a(i,k) \\leftarrow \\min\\big(0,\\; r(k,k) + \\sum_{i' \\notin \\{i,k\\}} \\max(0, r(i',k)) \\big) $\n",
    "- **preference** 값이 클수록 대표점(exemplar) 수 증가 경향 있음\n",
    "- **damping** $\\in [0.5, 1)$: 수렴 안정성 위해 도입함\n",
    "\n",
    "### 2.2 VADER 감성\n",
    "- `compound ∈ [-1, 1]` (전역 극성)\n",
    "- `pos/neg/neu` 비율도 같이 특성에 포함함\n",
    "\n",
    "### 2.3 SHAP (KernelExplainer)\n",
    "- 임의의 블랙박스 함수 $f(\\mathbf{x})$에 대해 Shapley 값 근사\n",
    "- 각 특성 $j$의 중요도 $\\phi_j$가 **기여도**로 해석됨\n",
    "- 회귀에서는 선형 합: $ f(\\mathbf{x}) \\approx E[f(\\mathbf{x})] + \\sum_j \\phi_j $\n",
    "\n",
    "### 2.4 Kano 매핑(단순 규칙)\n",
    "- 클러스터 관련 특성의 **평균 SHAP** 부호/크기/일관성으로 판정함\n",
    "- 예시 규칙(단순화):\n",
    "  - $|\\overline{\\phi}| \\approx 0$ → **Indifferent**\n",
    "  - $\\overline{\\phi} \\gg 0$ → **Attractive / One-dimensional**\n",
    "  - $\\overline{\\phi} \\ll 0$ → **Must-be / Reverse**\n",
    "- 실제 연구/프로덕션에서는 임계·신뢰구간·빈도 보정 등 더 정교화 권장함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fec0cf",
   "metadata": {},
   "source": [
    "## 3) 데이터 로딩: MARC (EN)\n",
    "- Hugging Face `amazon_reviews_multi`, 영어(en) 서브셋 사용함\n",
    "- 컬럼 예: `review_body`(본문), `review_title`(제목), `stars`(별점 1..5)\n",
    "- 노트: 이 셀은 인터넷 필요함 (로컬/Colab 등에서 실행 권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8389a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ✅ 원본(에러 발생): ds = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
    "ds = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")  # ← 미러 사용 (Parquet)\n",
    "\n",
    "def to_df(split):\n",
    "    d = ds[split]\n",
    "    # mteb 스키마: ['id','text','label','label_text']\n",
    "    # label: 0..4  → 별점(1..5)로 맞춤\n",
    "    return pd.DataFrame({\n",
    "        \"text\": d[\"text\"],\n",
    "        \"stars\": (np.array(d[\"label\"]) + 1).tolist()\n",
    "    })\n",
    "\n",
    "df_train = to_df(\"train\")\n",
    "df_test  = to_df(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5144a0",
   "metadata": {},
   "source": [
    "## 4) 전처리 & 토큰화\n",
    "- 공백 정리, 단어 토큰화, 불용어 제거, 알파벳 토큰만 사용\n",
    "- Word2Vec 학습 대비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5f5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize train: 100%|██████████| 200000/200000 [00:21<00:00, 9218.43it/s] \n",
      "Tokenize test: 100%|██████████| 5000/5000 [00:00<00:00, 9068.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_texts = [clean_text(t) for t in df_train[\"text\"].tolist()]\n",
    "test_texts  = [clean_text(t) for t in df_test[\"text\"].tolist()]\n",
    "\n",
    "tok_train = [tokenize_en(t) for t in tqdm(train_texts, desc=\"Tokenize train\")]\n",
    "tok_test  = [tokenize_en(t) for t in tqdm(test_texts,  desc=\"Tokenize test\")]\n",
    "\n",
    "len(tok_train), len(tok_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa70dd",
   "metadata": {},
   "source": [
    "## 5) Word2Vec 학습\n",
    "- 파라미터: `vector_size = cfg.w2v_size`, `window=5`, `min_count=2`\n",
    "- 코퍼스 전체에 대해 학습 후, 단어 벡터 취득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d58e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "ensure_nltk()\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1fe072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31166, 100),\n",
       " ['great',\n",
       "  'good',\n",
       "  'one',\n",
       "  'like',\n",
       "  'product',\n",
       "  'would',\n",
       "  'use',\n",
       "  'well',\n",
       "  'quality',\n",
       "  'get'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=tok_train,\n",
    "    vector_size=cfg.w2v_size,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    seed=cfg.random_state\n",
    ")\n",
    "\n",
    "vocab = list(w2v.wv.index_to_key)\n",
    "X = np.vstack([w2v.wv[w] for w in vocab])\n",
    "X.shape, vocab[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174239dd",
   "metadata": {},
   "source": [
    "## 6) Affinity Propagation(AP) 군집\n",
    "- 기본: 코사인 유사도 행렬 $S$ 계산 후 `affinity=\"precomputed\"`로 AP 수행\n",
    "- `preference`: 기본 median 사용 → 클러스터 수 자동 조절됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213efce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "벡터 준비 중: 100%|██████████| 31166/31166 [00:00<00:00, 1054531.13word/s]\n",
      "GPU로 데이터 전송 중: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "벡터 정규화 중: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "코사인 유사도 행렬 계산 중 (31166x31166): 100%|██████████| 1/1 [00:00<00:00,  5.48matrix/s]\n",
      "CPU로 결과 전송 중: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Preference 계산 중: 100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n",
      "Affinity Propagation 실행 중 (sklearn):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_clusters_with_AP(w2v, affinity=\"precomputed\", damping=0.7, preference=\"median\", random_state=42, use_gpu=True):\n",
    "    vocab = list(w2v.wv.index_to_key)\n",
    "    \n",
    "    # 1단계: 벡터 준비\n",
    "    with tqdm(total=len(vocab), desc=\"벡터 준비 중\", unit=\"word\") as pbar:\n",
    "        X = np.vstack([w2v.wv[w] for w in vocab])\n",
    "        pbar.update(len(vocab))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "    \n",
    "    if affinity == \"precomputed\":\n",
    "        # 2단계: GPU로 전송\n",
    "        with tqdm(total=1, desc=\"GPU로 데이터 전송 중\") as pbar:\n",
    "            X_torch = torch.from_numpy(X).float().to(device)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 3단계: 정규화\n",
    "        with tqdm(total=1, desc=\"벡터 정규화 중\") as pbar:\n",
    "            X_norm = torch.nn.functional.normalize(X_torch, p=2, dim=1)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 4단계: 코사인 유사도 행렬 계산\n",
    "        N = X.shape[0]\n",
    "        with tqdm(total=1, desc=f\"코사인 유사도 행렬 계산 중 ({N}x{N})\", unit=\"matrix\") as pbar:\n",
    "            S_gpu = torch.mm(X_norm, X_norm.T)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 5단계: CPU로 전송\n",
    "        with tqdm(total=1, desc=\"CPU로 결과 전송 중\") as pbar:\n",
    "            S = S_gpu.cpu().numpy()\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 6단계: Preference 계산\n",
    "        with tqdm(total=1, desc=\"Preference 계산 중\") as pbar:\n",
    "            if preference == \"median\":\n",
    "                pref = np.median(S)\n",
    "            elif preference == \"min\":\n",
    "                pref = np.min(S)\n",
    "            else:\n",
    "                pref = None\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 7단계: Affinity Propagation 실행\n",
    "        with tqdm(total=1, desc=\"Affinity Propagation 실행 중 (sklearn)\") as pbar:\n",
    "            ap = AffinityPropagation(affinity=\"precomputed\", damping=damping, preference=pref, random_state=random_state)\n",
    "            labels = ap.fit_predict(S)\n",
    "            pbar.update(1)\n",
    "    else:\n",
    "        # Euclidean 모드\n",
    "        with tqdm(total=1, desc=\"Affinity Propagation 실행 중 (Euclidean)\") as pbar:\n",
    "            ap = AffinityPropagation(affinity=\"euclidean\", damping=damping, random_state=random_state)\n",
    "            labels = ap.fit_predict(X)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # 8단계: 결과 정리\n",
    "    with tqdm(total=len(vocab), desc=\"클러스터 맵 생성 중\", unit=\"word\") as pbar:\n",
    "        cluster_vocab = {w: int(c) for w, c in zip(vocab, labels)}\n",
    "        pbar.update(len(vocab))\n",
    "    \n",
    "    n_clusters = len(set(labels))\n",
    "    \n",
    "    return cluster_vocab, n_clusters\n",
    "\n",
    "cluster_vocab, n_clusters = build_clusters_with_AP(\n",
    "    w2v, \n",
    "    affinity=cfg.ap_affinity, \n",
    "    damping=cfg.ap_damping, \n",
    "    preference=cfg.ap_preference, \n",
    "    random_state=cfg.random_state,\n",
    "    use_gpu=True  # GPU 사용 여부\n",
    ")\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb756ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_clusters_with_AP(w2v, affinity=\"precomputed\", damping=0.7, preference=\"median\", random_state=42):\n",
    "#     vocab = list(w2v.wv.index_to_key)\n",
    "#     X = np.vstack([w2v.wv[w] for w in vocab])\n",
    "\n",
    "#     if affinity == \"precomputed\":\n",
    "#         S = cosine_similarity(X)\n",
    "#         if preference == \"median\":\n",
    "#             pref = np.median(S)\n",
    "#         elif preference == \"min\":\n",
    "#             pref = np.min(S)\n",
    "#         else:\n",
    "#             pref = None\n",
    "#         ap = AffinityPropagation(affinity=\"precomputed\", damping=damping, preference=pref, random_state=random_state)\n",
    "#         labels = ap.fit_predict(S)\n",
    "#     else:\n",
    "#         ap = AffinityPropagation(affinity=\"euclidean\", damping=damping, random_state=random_state)\n",
    "#         labels = ap.fit_predict(X)\n",
    "\n",
    "#     cluster_vocab = {w: int(c) for w, c in zip(vocab, labels)}\n",
    "#     n_clusters = len(set(labels))\n",
    "#     return cluster_vocab, n_clusters\n",
    "\n",
    "# cluster_vocab, n_clusters = build_clusters_with_AP(\n",
    "#     w2v, affinity=cfg.ap_affinity, damping=cfg.ap_damping, preference=cfg.ap_preference, random_state=cfg.random_state\n",
    "# )\n",
    "# n_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8893abbe",
   "metadata": {},
   "source": [
    "## 7) 특징 벡터 구성\n",
    "- 클러스터 히스토그램(정규화) + VADER 감성 + 길이/문장부호\n",
    "- 결과: `[n_samples, n_clusters + 4 + 4]` 크기 특성 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd803845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def featurize_reviews(tokenized_docs, raw_texts, cluster_vocab, n_clusters, sia):\n",
    "    feats = []\n",
    "    for toks, raw in zip(tokenized_docs, raw_texts):\n",
    "        # 1) 클러스터 히스토그램\n",
    "        hist = np.zeros(n_clusters, dtype=float)\n",
    "        for t in toks:\n",
    "            cid = cluster_vocab.get(t, None)\n",
    "            if cid is not None and cid >= 0:\n",
    "                hist[cid] += 1.0\n",
    "        if hist.sum() > 0:\n",
    "            hist = hist / hist.sum()\n",
    "\n",
    "        # 2) 감성 (VADER)\n",
    "        s = sia.polarity_scores(raw)\n",
    "        sent = np.array([s['pos'], s['neg'], s['neu'], s['compound']], dtype=float)\n",
    "\n",
    "        # 3) 길이/문장부호\n",
    "        style = np.array([len(raw), len(toks), raw.count('!'), raw.count('?')], dtype=float)\n",
    "\n",
    "        feats.append(np.concatenate([hist, sent, style]))\n",
    "    return np.vstack(feats)\n",
    "\n",
    "X_train = featurize_reviews(tok_train, train_texts, cluster_vocab, n_clusters, sia)\n",
    "X_test  = featurize_reviews(tok_test,  test_texts,  cluster_vocab, n_clusters, sia)\n",
    "\n",
    "feature_names = [f\"cluster_{i}\" for i in range(n_clusters)] +                 [\"sent_pos\",\"sent_neg\",\"sent_neu\",\"sent_compound\",\"len_chars\",\"len_tokens\",\"exclaim_cnt\",\"question_cnt\"]\n",
    "\n",
    "X_train.shape, X_test.shape, feature_names[:8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f35d8",
   "metadata": {},
   "source": [
    "## 8) 신경망 학습 (MLP Regressor)\n",
    "- 타깃: 별점(1..5) → 회귀로 학습, 지표는 **MAE**\n",
    "- 파이프라인: `StandardScaler(with_mean=False) + MLPRegressor(128,64)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43409ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[\"stars\"].astype(float).to_numpy()\n",
    "y_test  = df_test[\"stars\"].astype(float).to_numpy()\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(128,64), activation=\"relu\",\n",
    "                         learning_rate_init=1e-3, random_state=cfg.random_state,\n",
    "                         early_stopping=True, validation_fraction=0.1, max_iter=200))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "print(f\"[MAE] test = {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e605f",
   "metadata": {},
   "source": [
    "## 9) SHAP (KernelExplainer)\n",
    "- 배경 샘플: 학습 특성에서 일부 샘플링\n",
    "- 회귀 모델 예측 함수를 그대로 사용\n",
    "- 요약 플롯 저장함(환경에 따라 노트북 내 표시도 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f245c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bg = shap.sample(X_train, min(100, X_train.shape[0]), random_state=cfg.random_state)\n",
    "\n",
    "def predict_fn(Xm):\n",
    "    return model.predict(Xm)\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_fn, bg)\n",
    "sample = X_test[: min(400, X_test.shape[0])]\n",
    "shap_vals = explainer.shap_values(sample, nsamples=200)  # (N_sample, F)\n",
    "\n",
    "# 요약 플롯\n",
    "shap.summary_plot(shap_vals, sample, feature_names=feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "plt.savefig(\"artifacts/shap_summary.png\", dpi=180)\n",
    "plt.close()\n",
    "\"artifacts/shap_summary.png 저장됨\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137770d1",
   "metadata": {},
   "source": [
    "## 10) Kano 규칙 매핑 (간단 버전)\n",
    "- 평균 SHAP 기반 단순 규칙으로 클러스터별 Kano 후보 판정\n",
    "- 실제 적용 시 임계/신뢰구간/빈도 보정 등 정교화 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fa69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kano_from_shap(shap_values, feature_names,\n",
    "                   cluster_prefix=\"cluster_\", neg_thresh=-0.02, pos_thresh=0.02):\n",
    "    mean_shap = np.mean(shap_values, axis=0)  # (F,)\n",
    "    kano_map = {}\n",
    "    for i, fn in enumerate(feature_names):\n",
    "        if fn.startswith(cluster_prefix):\n",
    "            v = float(mean_shap[i])\n",
    "            if abs(v) < 0.005:\n",
    "                kano = \"indifferent\"\n",
    "            elif v > pos_thresh:\n",
    "                kano = \"attractive/one-dimensional\"\n",
    "            elif v < neg_thresh:\n",
    "                kano = \"must-be/reverse\"\n",
    "            else:\n",
    "                kano = \"indifferent\"\n",
    "            kano_map[fn] = {\"mean_shap\": v, \"kano\": kano}\n",
    "    return kano_map\n",
    "\n",
    "kano_map = kano_from_shap(shap_vals, feature_names)\n",
    "kano_df = (pd.DataFrame.from_dict(kano_map, orient=\"index\")\n",
    "           .reset_index().rename(columns={\"index\":\"feature\"})\n",
    "           .sort_values(by=\"mean_shap\", ascending=False))\n",
    "kano_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583cb11",
   "metadata": {},
   "source": [
    "## 11) 산출물 저장\n",
    "- 모델 스택: `artifacts/model_ap.joblib` (파이프라인, Word2Vec, 군집 맵 등)\n",
    "- SHAP 값/플롯: `artifacts/shap_values.npy`, `artifacts/shap_summary.png`\n",
    "- Kano 테이블: `artifacts/kano_map.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artifacts = {\n",
    "    \"pipeline\": model,\n",
    "    \"w2v\": w2v,\n",
    "    \"cluster_vocab\": cluster_vocab,\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"feature_names\": feature_names\n",
    "}\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(artifacts, \"artifacts/model_ap.joblib\")\n",
    "np.save(\"artifacts/shap_values.npy\", shap_vals)\n",
    "with open(\"artifacts/kano_map.json\",\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {\"mean_shap\": float(v[\"mean_shap\"]), \"kano\": v[\"kano\"]} for k,v in kano_map.items()},\n",
    "              f, ensure_ascii=False, indent=2)\n",
    "[\"artifacts/model_ap.joblib\", \"artifacts/shap_values.npy\", \"artifacts/kano_map.json\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e723253",
   "metadata": {},
   "source": [
    "## 12) 다음 스텝\n",
    "- 한국어로 확장하려면: MARC-ko, 한국어 형태소 분석기/감성사전 교체 필요함\n",
    "- Kano 규칙 고도화: 평균±신뢰구간, 표본빈도 가중, 분포 비교, 다중가설 보정 적용 권장\n",
    "- AP vs X-means/다른 군집 비교: DBI, NMI, 안정성(다회 실행) 지표로 비교 가능함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
