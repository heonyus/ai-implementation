{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GqvhKS0IB199"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision timm datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ8onwTgGnfI",
        "outputId": "c80884ff-584e-4489-ecbf-14c222efd536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Downloading: https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n",
            "Extracting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\1339961556.py:31: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(root)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 9469 3925\n",
            "Classes: ['n01440764', 'n02102040', 'n02979186', 'n03000684', 'n03028079', 'n03394916', 'n03417042', 'n03425413', 'n03445777', 'n03888257']\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 0) 설치 (필요 시)\n",
        "# =========================================================\n",
        "# !pip install -q timm scikit-image\n",
        "\n",
        "# =========================================================\n",
        "# 1) 데이터: Imagenette 다운로드 → ImageFolder 로드\n",
        "# =========================================================\n",
        "import os, tarfile, urllib.request, pathlib, random\n",
        "import torch, numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "root = pathlib.Path(\"./data\")\n",
        "root.mkdir(exist_ok=True)\n",
        "tgz = root / \"imagenette2-320.tgz\"\n",
        "dst = root / \"imagenette2-320\"\n",
        "\n",
        "if not dst.exists():\n",
        "    if not tgz.exists():\n",
        "        url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\n",
        "        print(\"Downloading:\", url)\n",
        "        urllib.request.urlretrieve(url, tgz.as_posix())\n",
        "    print(\"Extracting...\")\n",
        "    with tarfile.open(tgz, \"r:gz\") as tf:\n",
        "        tf.extractall(root)\n",
        "\n",
        "train_dir = dst / \"train\"\n",
        "val_dir   = dst / \"val\"\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "train_set = datasets.ImageFolder(train_dir.as_posix(), transform=train_tf)\n",
        "val_set   = datasets.ImageFolder(val_dir.as_posix(),   transform=val_tf)\n",
        "BATCH_SIZE = 32\n",
        "NW = max(2, os.cpu_count() // 2) if os.cpu_count() else 2\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NW, pin_memory=True, persistent_workers=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NW, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_set), len(val_set))\n",
        "print(\"Classes:\", train_set.classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPIoFoKnJG1o",
        "outputId": "2e99c31d-bc57-4e2c-c242-e873d233e154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 2) 모델: timm ResNet-50 (로짓 반환)\n",
        "# =========================================================\n",
        "import timm\n",
        "model = timm.create_model(\"resnet50\", pretrained=True, num_classes=1000).eval().to(device)\n",
        "\n",
        "# 배치 하나 미리 가져오기\n",
        "xb_cpu, yb_cpu = next(iter(val_loader))\n",
        "xb = xb_cpu.to(device, non_blocking=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hMnVzOtgJKsB"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 3) 유틸: 디노멀라이즈 / 텐서→이미지\n",
        "# =========================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def denorm(img_t):\n",
        "    mean = torch.tensor(IMAGENET_MEAN, device=img_t.device).view(3,1,1)\n",
        "    std  = torch.tensor(IMAGENET_STD,  device=img_t.device).view(3,1,1)\n",
        "    return img_t * std + mean\n",
        "\n",
        "def to_numpy_img(img_t):\n",
        "    \"\"\"(C,H,W) normalized tensor -> (H,W,3) numpy [0,1]\"\"\"\n",
        "    img = denorm(img_t).clamp(0,1).permute(1,2,0).detach().cpu().numpy()\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QpBNw5sRJNAd"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 4) IG 미니 구현 (+Smooth-IG 옵션)\n",
        "# =========================================================\n",
        "from torch import nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def _alphas(steps:int, device):\n",
        "    # 중점 규칙 (midpoint)\n",
        "    return torch.linspace(0, 1, steps+1, device=device)[:-1] + 0.5/steps\n",
        "\n",
        "def integrated_gradients(\n",
        "    model: nn.Module,\n",
        "    x: torch.Tensor,               # (B,C,H,W)\n",
        "    target_idx: torch.Tensor,      # (B,)\n",
        "    baseline: torch.Tensor = None, # (B,C,H,W)\n",
        "    steps: int = 64,\n",
        "    chunk: int = 64,\n",
        "):\n",
        "    model.eval()\n",
        "    device = x.device\n",
        "    B = x.size(0)\n",
        "    if baseline is None:\n",
        "        baseline = torch.zeros_like(x)\n",
        "\n",
        "    diff = x - baseline\n",
        "    al = _alphas(steps, device)                         # (steps,)\n",
        "    total = torch.zeros_like(x, dtype=torch.float32)    # (B,C,H,W)\n",
        "\n",
        "    for s in range(0, steps, chunk):\n",
        "        a = al[s: s+chunk].view(-1, *([1]*x.dim()))     # (S,1,1,1)\n",
        "        x_path = (baseline.unsqueeze(0) + a * diff.unsqueeze(0)).clone()\n",
        "        x_path.requires_grad_(True)                     # (S,B,C,H,W)\n",
        "        SB = x_path.shape[0] * x_path.shape[1]\n",
        "\n",
        "        y = model(x_path.view(SB, *x.shape[1:]))       # (S*B, num_classes)\n",
        "        y = y.view(x_path.shape[0], x_path.shape[1], -1)   # (S,B,C_cls)\n",
        "        tgt = target_idx.view(1,B).expand(y.size(0),B)      # (S,B)\n",
        "        f = y.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)     # (S,B)\n",
        "\n",
        "        grads = torch.autograd.grad(outputs=f.sum(), inputs=x_path,\n",
        "                                    create_graph=False, retain_graph=False, allow_unused=False)[0]  # (S,B,C,H,W)\n",
        "        avg_grads = grads.mean(dim=0)                              # (B,C,H,W)\n",
        "        total += avg_grads\n",
        "        del x_path, y, tgt, f, grads, avg_grads\n",
        "\n",
        "    ig = diff * (total / (steps / chunk))\n",
        "    return ig\n",
        "\n",
        "def smooth_ig(\n",
        "    model, x, target_idx, baseline=None, steps=64, n_samples=8, sigma=0.08, chunk=64\n",
        "):\n",
        "    if baseline is None: baseline = torch.zeros_like(x)\n",
        "    acc = torch.zeros_like(x)\n",
        "    for _ in range(n_samples):\n",
        "        eps = torch.randn_like(x) * sigma\n",
        "        acc += integrated_gradients(model, x+eps, target_idx, baseline+eps, steps=steps, chunk=chunk)\n",
        "    return acc / n_samples\n",
        "\n",
        "def completeness_check(model, x, baseline, target_idx):\n",
        "    with torch.no_grad():\n",
        "        fx = model(x).gather(-1, target_idx.view(-1,1)).squeeze(-1)\n",
        "        f0 = model(baseline).gather(-1, target_idx.view(-1,1)).squeeze(-1)\n",
        "    return fx - f0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V_uCV9vDJPDQ"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 5) IG → 단일맵 변환 / 정규화 / 마스크 / 컨투어\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "from skimage import measure, segmentation\n",
        "\n",
        "def ig_reduce(ig_sample: torch.Tensor, how=\"sum_abs\"):\n",
        "    # (C,H,W) -> (H,W)\n",
        "    if how == \"sum_abs\":\n",
        "        m = ig_sample.abs().sum(dim=0)\n",
        "    elif how == \"l2\":\n",
        "        m = torch.linalg.vector_norm(ig_sample, dim=0)\n",
        "    elif how == \"sum\":\n",
        "        m = ig_sample.sum(dim=0)\n",
        "    else:\n",
        "        raise ValueError(how)\n",
        "    return m\n",
        "\n",
        "def normalize_01(m: torch.Tensor, clip_p=0.99):\n",
        "    v = m.flatten()\n",
        "    hi = torch.quantile(v, clip_p).clamp(min=1e-8)\n",
        "    m = m.clamp(0, hi) / hi\n",
        "    return m\n",
        "\n",
        "def threshold_percentile(m01: torch.Tensor, keep=0.15):\n",
        "    thr = torch.quantile(m01.flatten(), 1 - keep)\n",
        "    mask = (m01 >= thr).float()\n",
        "    return mask, float(thr)\n",
        "\n",
        "def draw_contours_on(ax, m01: torch.Tensor, level=0.6, color=\"lime\", lw=1.8):\n",
        "    arr = m01.cpu().numpy()\n",
        "    cs = measure.find_contours(arr, level=level)\n",
        "    for c in cs:\n",
        "        ax.plot(c[:,1], c[:,0], color=color, linewidth=lw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZBjNizYZJQ0V"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.18 GiB is allocated by PyTorch, and 81.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m baseline = torch.zeros_like(xb)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 표준 IG 또는 Smooth-IG 중 택1\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ig = integrated_gradients(model, xb, target_idx=top1_idx, baseline=baseline, steps=128, chunk=64)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ig = \u001b[43msmooth_ig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop1_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.06\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m lhs = ig.view(ig.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m).sum(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m rhs = completeness_check(model, xb, baseline, top1_idx)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36msmooth_ig\u001b[39m\u001b[34m(model, x, target_idx, baseline, steps, n_samples, sigma, chunk)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[32m     55\u001b[39m     eps = torch.randn_like(x) * sigma\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     acc += \u001b[43mintegrated_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m+\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m+\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m acc / n_samples\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mintegrated_gradients\u001b[39m\u001b[34m(model, x, target_idx, baseline, steps, chunk)\u001b[39m\n\u001b[32m     32\u001b[39m x_path.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)                     \u001b[38;5;66;03m# (S,B,C,H,W)\u001b[39;00m\n\u001b[32m     33\u001b[39m SB = x_path.shape[\u001b[32m0\u001b[39m] * x_path.shape[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m y = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# (S*B, num_classes)\u001b[39;00m\n\u001b[32m     36\u001b[39m y = y.view(x_path.shape[\u001b[32m0\u001b[39m], x_path.shape[\u001b[32m1\u001b[39m], -\u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# (S,B,C_cls)\u001b[39;00m\n\u001b[32m     37\u001b[39m tgt = target_idx.view(\u001b[32m1\u001b[39m,B).expand(y.size(\u001b[32m0\u001b[39m),B)      \u001b[38;5;66;03m# (S,B)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\timm\\models\\resnet.py:775\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    774\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    776\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\timm\\models\\resnet.py:747\u001b[39m, in \u001b[36mResNet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    745\u001b[39m x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    746\u001b[39m x = \u001b[38;5;28mself\u001b[39m.act1(x)\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n\u001b[32m    750\u001b[39m     x = checkpoint_seq([\u001b[38;5;28mself\u001b[39m.layer1, \u001b[38;5;28mself\u001b[39m.layer2, \u001b[38;5;28mself\u001b[39m.layer3, \u001b[38;5;28mself\u001b[39m.layer4], x, flatten=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m    225\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:627\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:823\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    822\u001b[39m     stride = torch.jit.annotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.18 GiB is allocated by PyTorch, and 81.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 6) 한 배치에 대해: 타깃 선택 → IG 계산(Smooth-IG 권장)\n",
        "# =========================================================\n",
        "with torch.no_grad():\n",
        "    logits = model(xb)                           # (B,1000)\n",
        "top1_idx = logits.softmax(-1).argmax(-1)         # (B,)\n",
        "baseline = torch.zeros_like(xb)\n",
        "\n",
        "# 표준 IG 또는 Smooth-IG 중 택1\n",
        "# ig = integrated_gradients(model, xb, target_idx=top1_idx, baseline=baseline, steps=128, chunk=64)\n",
        "ig = smooth_ig(model, xb, target_idx=top1_idx, baseline=baseline, steps=128, n_samples=8, sigma=0.06, chunk=64)\n",
        "\n",
        "lhs = ig.view(ig.size(0), -1).sum(dim=1)\n",
        "rhs = completeness_check(model, xb, baseline, top1_idx)\n",
        "print(\"Completeness gap (mean±std):\", float((lhs - rhs).abs().mean()), float((lhs - rhs).abs().std()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH2TdSInJSny"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 7) 시각화 5종: 원본 / heat-only / 원본+overlay / 원본+Top-k mask / 원본+contour\n",
        "# =========================================================\n",
        "def show_panel(x_i, ig_i, keep=0.15, clip_p=0.99, overlay_alpha=0.45, cmap=\"magma\", contour_level=0.6):\n",
        "    \"\"\"\n",
        "    x_i: (C,H,W) normalized image\n",
        "    ig_i: (C,H,W) IG\n",
        "    \"\"\"\n",
        "    img = to_numpy_img(x_i)\n",
        "    m   = ig_reduce(ig_i, \"sum_abs\")\n",
        "    m01 = normalize_01(m, clip_p=clip_p)\n",
        "    mask,_ = threshold_percentile(m01, keep=keep)\n",
        "\n",
        "    fig, axs = plt.subplots(1,5, figsize=(15,3))\n",
        "    # (1) 원본\n",
        "    axs[0].imshow(img); axs[0].set_title(\"Original\"); axs[0].axis(\"off\")\n",
        "    # (2) heat-only\n",
        "    axs[1].imshow(m01.cpu().numpy(), cmap=cmap, vmin=0, vmax=1); axs[1].set_title(\"Heat-only\"); axs[1].axis(\"off\")\n",
        "    # (3) 원본 + overlay\n",
        "    axs[2].imshow(img); axs[2].imshow(m01.cpu().numpy(), cmap=cmap, alpha=overlay_alpha, vmin=0, vmax=1)\n",
        "    axs[2].set_title(\"Overlay\"); axs[2].axis(\"off\")\n",
        "    # (4) 원본 + Top-k mask(하얀색)\n",
        "    axs[3].imshow(img); axs[3].imshow(mask.cpu().numpy(), cmap=\"gray\", alpha=0.45, vmin=0, vmax=1)\n",
        "    axs[3].set_title(f\"Top-{int(keep*100)}% mask\"); axs[3].axis(\"off\")\n",
        "    # (5) 원본 + 컨투어\n",
        "    axs[4].imshow(img); draw_contours_on(axs[4], m01, level=contour_level, color=\"lime\", lw=2.0)\n",
        "    axs[4].set_title(f\"Contour @{contour_level:.2f}\"); axs[4].axis(\"off\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# 배치의 앞 4장만 시각화\n",
        "for i in range(min(4, xb.size(0))):\n",
        "    show_panel(xb[i], ig[i], keep=0.15, clip_p=0.99, overlay_alpha=0.45, cmap=\"magma\", contour_level=0.6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a32rj-kRJW5V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
