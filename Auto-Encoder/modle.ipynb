{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076e9a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 12.6/241.3 MB 60.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 17.0/241.3 MB 43.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 22.5/241.3 MB 35.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 34.6/241.3 MB 42.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 43.5/241.3 MB 42.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 56.4/241.3 MB 45.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 68.7/241.3 MB 47.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 81.8/241.3 MB 49.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 94.1/241.3 MB 50.5 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 106.4/241.3 MB 51.5 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 117.2/241.3 MB 51.6 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 122.9/241.3 MB 49.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 135.0/241.3 MB 50.1 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 143.7/241.3 MB 49.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 156.8/241.3 MB 50.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 163.8/241.3 MB 49.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 176.2/241.3 MB 50.0 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 188.5/241.3 MB 50.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 200.8/241.3 MB 51.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 212.9/241.3 MB 51.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 225.4/241.3 MB 52.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  237.8/241.3 MB 52.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  241.2/241.3 MB 52.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 241.3/241.3 MB 33.3 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 43.2 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 57.2 MB/s eta 0:00:00\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision scikit-learn matplotlib numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d616ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretrain] RBM 1/4: 784 -> 1000 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.1812\n",
      "  Epoch 02 | recon_mse=0.1538\n",
      "  Epoch 03 | recon_mse=0.1509\n",
      "  Epoch 04 | recon_mse=0.1474\n",
      "  Epoch 05 | recon_mse=0.1421\n",
      "[Pretrain] RBM 2/4: 1000 -> 500 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2489\n",
      "  Epoch 02 | recon_mse=0.2467\n",
      "  Epoch 03 | recon_mse=0.2466\n",
      "  Epoch 04 | recon_mse=0.2463\n",
      "  Epoch 05 | recon_mse=0.2453\n",
      "[Pretrain] RBM 3/4: 500 -> 250 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2344\n",
      "  Epoch 02 | recon_mse=0.2182\n",
      "  Epoch 03 | recon_mse=0.2156\n",
      "  Epoch 04 | recon_mse=0.2145\n",
      "  Epoch 05 | recon_mse=0.2138\n",
      "[Pretrain] RBM 4/4: 250 -> 30 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2835\n",
      "  Epoch 02 | recon_mse=0.2724\n",
      "  Epoch 03 | recon_mse=0.2648\n",
      "  Epoch 04 | recon_mse=0.2582\n",
      "  Epoch 05 | recon_mse=0.2523\n",
      "[Fine-tune] start\n",
      "[Fine-tune] Epoch 01 | BCE=0.2407\n",
      "[Fine-tune] Epoch 02 | BCE=0.2319\n",
      "[Fine-tune] Epoch 03 | BCE=0.2252\n",
      "[Fine-tune] Epoch 04 | BCE=0.2189\n",
      "[Fine-tune] Epoch 05 | BCE=0.2150\n",
      "[Fine-tune] Epoch 06 | BCE=0.2119\n",
      "[Fine-tune] Epoch 07 | BCE=0.2094\n",
      "[Fine-tune] Epoch 08 | BCE=0.2070\n",
      "[Fine-tune] Epoch 09 | BCE=0.2049\n",
      "[Fine-tune] Epoch 10 | BCE=0.2036\n",
      "[Test] Recon MSE — DAE: 0.047665 | PCA(k=30): 0.017687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synth Curves: 100%|██████████| 20000/20000 [00:01<00:00, 10644.82it/s]\n",
      "Synth Curves: 100%|██████████| 10000/10000 [00:00<00:00, 10901.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretrain] RBM 1/6: 784 -> 400 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2419\n",
      "  Epoch 02 | recon_mse=0.1148\n",
      "  Epoch 03 | recon_mse=0.0783\n",
      "  Epoch 04 | recon_mse=0.0616\n",
      "  Epoch 05 | recon_mse=0.0526\n",
      "[Pretrain] RBM 2/6: 400 -> 200 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2504\n",
      "  Epoch 02 | recon_mse=0.2494\n",
      "  Epoch 03 | recon_mse=0.2492\n",
      "  Epoch 04 | recon_mse=0.2491\n",
      "  Epoch 05 | recon_mse=0.2491\n",
      "[Pretrain] RBM 3/6: 200 -> 100 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2524\n",
      "  Epoch 02 | recon_mse=0.2486\n",
      "  Epoch 03 | recon_mse=0.2471\n",
      "  Epoch 04 | recon_mse=0.2462\n",
      "  Epoch 05 | recon_mse=0.2456\n",
      "[Pretrain] RBM 4/6: 100 -> 50 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2622\n",
      "  Epoch 02 | recon_mse=0.2540\n",
      "  Epoch 03 | recon_mse=0.2495\n",
      "  Epoch 04 | recon_mse=0.2469\n",
      "  Epoch 05 | recon_mse=0.2449\n",
      "[Pretrain] RBM 5/6: 50 -> 25 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2667\n",
      "  Epoch 02 | recon_mse=0.2597\n",
      "  Epoch 03 | recon_mse=0.2547\n",
      "  Epoch 04 | recon_mse=0.2517\n",
      "  Epoch 05 | recon_mse=0.2491\n",
      "[Pretrain] RBM 6/6: 25 -> 6 (bernoulli -> bernoulli)\n",
      "  Epoch 01 | recon_mse=0.2611\n",
      "  Epoch 02 | recon_mse=0.2593\n",
      "  Epoch 03 | recon_mse=0.2576\n",
      "  Epoch 04 | recon_mse=0.2559\n",
      "  Epoch 05 | recon_mse=0.2550\n",
      "[Fine-tune] start\n",
      "[Fine-tune] Epoch 01 | BCE=0.0960\n",
      "[Fine-tune] Epoch 02 | BCE=0.0958\n",
      "[Fine-tune] Epoch 03 | BCE=0.0957\n",
      "[Fine-tune] Epoch 04 | BCE=0.0957\n",
      "[Fine-tune] Epoch 05 | BCE=0.0957\n",
      "[Fine-tune] Epoch 06 | BCE=0.0957\n",
      "[Fine-tune] Epoch 07 | BCE=0.0957\n",
      "[Fine-tune] Epoch 08 | BCE=0.0956\n",
      "[Fine-tune] Epoch 09 | BCE=0.0956\n",
      "[Fine-tune] Epoch 10 | BCE=0.0957\n",
      "[Test] Recon MSE — DAE: 0.007794 | PCA(k=6): 0.004982\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Hinton & Salakhutdinov (Science, 2006) 재현 미니멀 구현\n",
    "- RBM(제한 볼츠만 기계) 층별 사전학습(Pretraining)\n",
    "- Autoencoder(오토인코더) 전개(Unroll) + 미세조정(Fine-tuning)\n",
    "- Curves(합성 데이터) & MNIST 실험\n",
    "- PCA(주성분 분석) 비교 (재구성 MSE)\n",
    "\n",
    "주의: 데모용으로 학습 에폭 수를 줄였음. 결과 재현도를 높이려면 에폭/배치/네트워크/옵티마이저 설정을 늘려서 돌려보세요.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utils\n",
    "# ---------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) RBM (Bernoulli-Bernoulli, Gaussian-Bernoulli 지원)\n",
    "# ---------------------------\n",
    "class RBM(nn.Module):\n",
    "    \"\"\"\n",
    "    Restricted Boltzmann Machine (RBM)\n",
    "    - visible_type: 'bernoulli' 또는 'gaussian'\n",
    "    - hidden_type : 'bernoulli'\n",
    "    Contrastive Divergence(CD-k)로 학습\n",
    "    \"\"\"\n",
    "    def __init__(self, n_visible, n_hidden, visible_type='bernoulli', k=1):\n",
    "        super().__init__()\n",
    "        assert visible_type in ['bernoulli', 'gaussian']\n",
    "        self.n_v = n_visible\n",
    "        self.n_h = n_hidden\n",
    "        self.visible_type = visible_type\n",
    "        self.k = k\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(n_visible, n_hidden) * 0.01)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n",
    "\n",
    "        # 가우시안 가시 유닛용 표준편차(여기선 1로 고정; 학습하려면 파라미터화 가능)\n",
    "        self.sigma = 1.0\n",
    "\n",
    "    def sample_h(self, v):\n",
    "        # P(h=1|v)\n",
    "        p = torch.sigmoid(F.linear(v, self.W.t(), self.h_bias))\n",
    "        return p, torch.bernoulli(p)\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        if self.visible_type == 'bernoulli':\n",
    "            p = torch.sigmoid(F.linear(h, self.W, self.v_bias))\n",
    "            return p, torch.bernoulli(p)\n",
    "        else:\n",
    "            # Gaussian visible units: mean = W h + b, sigma=1\n",
    "            mean = F.linear(h, self.W, self.v_bias)\n",
    "            v = mean + torch.randn_like(mean) * self.sigma\n",
    "            return mean, v  # 확률적 샘플도 반환\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        vbias_term = (v * self.v_bias).sum(dim=1)\n",
    "        wx_b = F.linear(v, self.W.t(), self.h_bias)\n",
    "        hidden_term = torch.log1p(torch.exp(wx_b)).sum(dim=1)\n",
    "        return -(vbias_term + hidden_term)\n",
    "\n",
    "    def forward(self, v):\n",
    "        # reconstruction (one-step)\n",
    "        ph, h = self.sample_h(v)\n",
    "        pv, v_sample = self.sample_v(h)\n",
    "        return pv, ph\n",
    "\n",
    "    def cd_loss(self, v0):\n",
    "        # CD-k\n",
    "        ph0, h0 = self.sample_h(v0)\n",
    "        vk = v0\n",
    "        hk = h0\n",
    "        for _ in range(self.k):\n",
    "            pvk, vk = self.sample_v(hk)\n",
    "            phk, hk = self.sample_h(vk)\n",
    "\n",
    "        # Positive / Negative phase\n",
    "        w_pos = torch.einsum('bi,bj->ij', v0, ph0)\n",
    "        w_neg = torch.einsum('bi,bj->ij', vk, phk)\n",
    "\n",
    "        loss = (self.free_energy(v0).mean() - self.free_energy(vk).mean())\n",
    "        # weight/bias 업데이트는 optimizer로 처리; 여기선 loss만 반환\n",
    "        stats = {\n",
    "            'recon_mse': F.mse_loss(vk, v0).item(),\n",
    "            'pos_mean_h': ph0.mean().item(),\n",
    "            'neg_mean_h': phk.mean().item(),\n",
    "        }\n",
    "        grads = {\n",
    "            'W_pos': w_pos / v0.size(0),\n",
    "            'W_neg': w_neg / v0.size(0),\n",
    "            'v_bias_pos': v0.mean(dim=0),\n",
    "            'v_bias_neg': vk.mean(dim=0),\n",
    "            'h_bias_pos': ph0.mean(dim=0),\n",
    "            'h_bias_neg': phk.mean(dim=0),\n",
    "        }\n",
    "        return loss, stats, grads\n",
    "\n",
    "    def manual_step(self, grads, lr=1e-3):\n",
    "        with torch.no_grad():\n",
    "            self.W += lr * (grads['W_pos'] - grads['W_neg'])\n",
    "            self.v_bias += lr * (grads['v_bias_pos'] - grads['v_bias_neg'])\n",
    "            self.h_bias += lr * (grads['h_bias_pos'] - grads['h_bias_neg'])\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) RBM 스택 사전학습\n",
    "# ---------------------------\n",
    "def pretrain_rbm_stack(data_loader, layer_sizes: List[int],\n",
    "                       visible_type_first='bernoulli', k=1, epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    layer_sizes: [n_visible, h1, h2, ...] 형태\n",
    "    \"\"\"\n",
    "    rbms = []\n",
    "    data_rep = None\n",
    "\n",
    "    for li in range(len(layer_sizes) - 1):\n",
    "        n_v = layer_sizes[li]\n",
    "        n_h = layer_sizes[li + 1]\n",
    "        vtype = visible_type_first if li == 0 else 'bernoulli'\n",
    "        rbm = RBM(n_v, n_h, visible_type=vtype, k=k).to(device)\n",
    "\n",
    "        print(f\"[Pretrain] RBM {li+1}/{len(layer_sizes)-1}: {n_v} -> {n_h} ({vtype} -> bernoulli)\")\n",
    "        for ep in range(1, epochs + 1):\n",
    "            meters = []\n",
    "            for batch in data_loader:\n",
    "                v0 = batch[0].to(device)\n",
    "                # 첫 층이 아니라면, 이전 층 RBM의 은닉 확률을 '데이터'로 사용\n",
    "                if data_rep is not None:\n",
    "                    with torch.no_grad():\n",
    "                        v0 = data_rep(v0)\n",
    "\n",
    "                loss, stats, grads = rbm.cd_loss(v0)\n",
    "                rbm.manual_step(grads, lr=lr)\n",
    "                meters.append(stats['recon_mse'])\n",
    "            print(f\"  Epoch {ep:02d} | recon_mse={np.mean(meters):.4f}\")\n",
    "\n",
    "        # 다음 층 입력으로 사용할 함수(은닉 확률)\n",
    "        def make_rep(rbm_):\n",
    "            def rep(v):\n",
    "                p_h, _ = rbm_.sample_h(v)\n",
    "                return p_h\n",
    "            return rep\n",
    "\n",
    "        rep_fn = make_rep(rbm)\n",
    "        # data_rep를 체인으로 누적\n",
    "        data_rep = (lambda f_old, f_new: (lambda x: f_new(f_old(x))))(data_rep, rep_fn) if data_rep else rep_fn\n",
    "        rbms.append(rbm)\n",
    "\n",
    "    return rbms\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) RBM 가중치로 Autoencoder 초기화 + 미세조정\n",
    "# ---------------------------\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RBM 스택을 Encoder로 사용하고, Transpose Weight(가중치 전치)를 Decoder에 공유하지는 않지만\n",
    "    초기값을 대칭으로 설정. (원 논문 컨셉: 전개 후 역전파 미세조정)\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes: List[int], code_linear=False):\n",
    "        super().__init__()\n",
    "        enc_layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            enc_layers += [nn.Linear(layer_sizes[i], layer_sizes[i+1])]\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                enc_layers += [nn.Sigmoid()]  # 로지스틱 유닛\n",
    "            else:\n",
    "                enc_layers += [nn.Identity() if code_linear else nn.Sigmoid()]\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "\n",
    "        dec_layers = []\n",
    "        for i in range(len(layer_sizes) - 1, 0, -1):\n",
    "            dec_layers += [nn.Linear(layer_sizes[i], layer_sizes[i-1])]\n",
    "            if i-1 > 0:\n",
    "                dec_layers += [nn.Sigmoid()]\n",
    "            else:\n",
    "                dec_layers += [nn.Sigmoid()]  # [0,1] 범위 재구성\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "\n",
    "def init_from_rbms(model: DeepAutoencoder, rbms: List[RBM]):\n",
    "    # 1) Encoder: v->h 이므로 Linear(out=n_h, in=n_v) = RBM.W.T\n",
    "    e_idx = 0\n",
    "    for m in model.encoder.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            W = rbms[e_idx].W.detach().t().clone()      # [n_h, n_v]\n",
    "            b = rbms[e_idx].h_bias.detach().clone()     # hidden bias\n",
    "            m.weight.data.copy_(W)\n",
    "            m.bias.data.copy_(b)\n",
    "            e_idx += 1\n",
    "\n",
    "    # 2) Decoder: h->v 이므로 Linear(out=n_v, in=n_h) = RBM.W  (전치 금지!)\n",
    "    d_linears = [m for m in model.decoder.modules() if isinstance(m, nn.Linear)]\n",
    "    for i, lin in enumerate(d_linears):\n",
    "        rbm = rbms[-(i+1)]                              # 역순으로 매칭\n",
    "        W = rbm.W.detach().clone()                      # [n_v, n_h] → Linear(out=n_v, in=n_h)\n",
    "        b = rbm.v_bias.detach().clone()                 # visible bias\n",
    "        # ✅ 기존의 .t() 를 제거하세요\n",
    "        lin.weight.data.copy_(W)\n",
    "        lin.bias.data.copy_(b)\n",
    "\n",
    "\n",
    "def fine_tune_autoencoder(model, data_loader, epochs=20, lr=1e-3, loss_type='bce'):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        losses = []\n",
    "        for (x,) in data_loader:\n",
    "            x = x.to(device)\n",
    "            x_hat, _ = model(x)\n",
    "            if loss_type == 'bce':\n",
    "                loss = F.binary_cross_entropy(x_hat, x)\n",
    "            else:\n",
    "                loss = F.mse_loss(x_hat, x)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "        mean_loss = float(np.mean(losses))\n",
    "        history.append(mean_loss)\n",
    "        print(f\"[Fine-tune] Epoch {ep:02d} | {loss_type.upper()}={mean_loss:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) 데이터셋: Curves(합성), MNIST\n",
    "# ---------------------------\n",
    "def make_curves_dataset(n_train=20000, n_test=10000, img_size=28):\n",
    "    \"\"\"\n",
    "    원 논문 'Curves' 합성 아이디어를 단순화:\n",
    "    - 2D에 랜덤 제어점 3개로 베지어/폴리라인 곡선 생성 후, 래스터라이즈\n",
    "    - 결과는 [0,1] intensity의 28x28 이미지\n",
    "    \"\"\"\n",
    "    def draw_curve(seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        p = np.random.rand(3, 2) * 0.8 + 0.1  # 0.1~0.9 범위\n",
    "        # 샘플 포인트\n",
    "        t = np.linspace(0, 1, 60)\n",
    "        curve = (1-t)[:,None]*(1-t)[:,None]*p[0] + 2*(1-t)[:,None]*t[:,None]*p[1] + t[:,None]*t[:,None]*p[2]\n",
    "        # 렌더링\n",
    "        img = np.zeros((img_size, img_size))\n",
    "        xy = (curve * (img_size-1)).astype(int)\n",
    "        for (x, y) in xy:\n",
    "            img[y, x] = 1.0\n",
    "        # 두께/블러\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        img = gaussian_filter(img, sigma=0.8)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "\n",
    "    def build(n):\n",
    "        xs = []\n",
    "        for _ in tqdm(range(n), desc=\"Synth Curves\"):\n",
    "            xs.append(draw_curve())\n",
    "        X = np.stack(xs).astype(np.float32)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        return X\n",
    "\n",
    "    Xtr = build(n_train)\n",
    "    Xte = build(n_test)\n",
    "    return Xtr, Xte\n",
    "\n",
    "\n",
    "def make_mnist_dataset(binarize=False):\n",
    "    tfms = transforms.Compose([\n",
    "        transforms.ToTensor(),  # [0,1]\n",
    "    ])\n",
    "    train = datasets.MNIST(root='./data', train=True, download=True, transform=tfms)\n",
    "    test  = datasets.MNIST(root='./data', train=False, download=True, transform=tfms)\n",
    "\n",
    "    def to_matrix(ds):\n",
    "        X = ds.data.numpy().astype(np.float32) / 255.0\n",
    "        X = X.reshape(len(X), -1)\n",
    "        if binarize:\n",
    "            X = (X > 0.5).astype(np.float32)\n",
    "        return X, ds.targets.numpy()\n",
    "\n",
    "    Xtr, ytr = to_matrix(train); Xte, yte = to_matrix(test)\n",
    "    return Xtr, ytr, Xte, yte\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5) 평가: 재구성 MSE & 2D 시각화\n",
    "# ---------------------------\n",
    "def recon_mse(model, X, batch=512):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot = 0.0; n = 0\n",
    "        for i in range(0, len(X), batch):\n",
    "            xb = torch.from_numpy(X[i:i+batch]).to(device)\n",
    "            xh, _ = model(xb)\n",
    "            tot += F.mse_loss(xh, xb, reduction='sum').item()\n",
    "            n += xb.numel()\n",
    "        return tot / (len(X)*X.shape[1])\n",
    "\n",
    "def pca_recon_mse(X_train, X_test, k):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X_train)\n",
    "    Xh = pca.inverse_transform(pca.transform(X_test))\n",
    "    return ((X_test - Xh) ** 2).mean()\n",
    "\n",
    "def plot_2d_codes(model, X, y=None, nsamp=5000, title=\"2D codes\"):\n",
    "    model.eval()\n",
    "    idx = np.random.choice(len(X), size=min(nsamp, len(X)), replace=False)\n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(X[idx]).to(device)\n",
    "        _, z = model(xb)\n",
    "        z = z[:, :2].detach().cpu().numpy() if z.shape[1] > 2 else z.detach().cpu().numpy()\n",
    "    plt.figure()\n",
    "    if y is not None:\n",
    "        yt = y[idx]\n",
    "        scatter = plt.scatter(z[:,0], z[:,1], c=yt, s=5, alpha=0.7)\n",
    "        plt.legend(*scatter.legend_elements(num=10), title=\"class\", loc=\"upper right\")\n",
    "    else:\n",
    "        plt.scatter(z[:,0], z[:,1], s=5, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"z1\"); plt.ylabel(\"z2\"); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6) 메인 실험 루틴\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class ExpConfig:\n",
    "    dataset: str = \"mnist\"       # 'mnist' or 'curves'\n",
    "    layer_sizes: Tuple[int,...] = (784, 1000, 500, 250, 30)  # 논문 예시\n",
    "    batch_size: int = 128\n",
    "    rbm_epochs: int = 5          # 데모용 (논문처럼 하려면 30+ 추천)\n",
    "    rbm_k: int = 1\n",
    "    rbm_lr: float = 1e-3\n",
    "    ft_epochs: int = 10          # 데모용\n",
    "    ft_lr: float = 1e-3\n",
    "    loss_type: str = \"bce\"       # 'bce' for [0,1] logistic, 'mse' otherwise\n",
    "    binarize_mnist: bool = False\n",
    "    code_linear: bool = True     # 상위 코드 선형(논문 비교 편의)\n",
    "\n",
    "\n",
    "def run_experiment(cfg: ExpConfig):\n",
    "    set_seed(7)\n",
    "\n",
    "    # 데이터 준비\n",
    "    if cfg.dataset == \"mnist\":\n",
    "        Xtr, ytr, Xte, yte = make_mnist_dataset(binarize=cfg.binarize_mnist)\n",
    "        input_dim = 784\n",
    "        assert cfg.layer_sizes[0] == input_dim, \"layer_sizes 입력 차원(784) 확인\"\n",
    "        train_loader = DataLoader(TensorDataset(torch.from_numpy(Xtr)), batch_size=cfg.batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(TensorDataset(torch.from_numpy(Xte)), batch_size=cfg.batch_size, shuffle=False)\n",
    "        visible_type_first = 'bernoulli'  # 픽셀 [0,1] → 로지스틱\n",
    "    elif cfg.dataset == \"curves\":\n",
    "        Xtr, Xte = make_curves_dataset(n_train=20000, n_test=10000, img_size=28)\n",
    "        input_dim = 784\n",
    "        assert cfg.layer_sizes[0] == input_dim\n",
    "        train_loader = DataLoader(TensorDataset(torch.from_numpy(Xtr)), batch_size=cfg.batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(TensorDataset(torch.from_numpy(Xte)), batch_size=cfg.batch_size, shuffle=False)\n",
    "        visible_type_first = 'bernoulli'  # 곡선 intensity [0,1] → 로지스틱\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'mnist' or 'curves'\")\n",
    "\n",
    "    # RBM 스택 사전학습\n",
    "    rbms = pretrain_rbm_stack(\n",
    "        data_loader=train_loader,\n",
    "        layer_sizes=list(cfg.layer_sizes),\n",
    "        visible_type_first=visible_type_first,\n",
    "        k=cfg.rbm_k, epochs=cfg.rbm_epochs, lr=cfg.rbm_lr\n",
    "    )\n",
    "\n",
    "    # Autoencoder 구성 & 초기화\n",
    "    dae = DeepAutoencoder(list(cfg.layer_sizes), code_linear=cfg.code_linear)\n",
    "    init_from_rbms(dae, rbms)\n",
    "\n",
    "    # 미세조정\n",
    "    print(\"[Fine-tune] start\")\n",
    "    fine_tune_autoencoder(dae, train_loader, epochs=cfg.ft_epochs, lr=cfg.ft_lr, loss_type=cfg.loss_type)\n",
    "\n",
    "    # 평가: 재구성 MSE (테스트)\n",
    "    mse_dae = recon_mse(dae, Xte, batch=512)\n",
    "    mse_pca = pca_recon_mse(Xtr, Xte, k=cfg.layer_sizes[-1])\n",
    "    print(f\"[Test] Recon MSE — DAE: {mse_dae:.6f} | PCA(k={cfg.layer_sizes[-1]}): {mse_pca:.6f}\")\n",
    "\n",
    "    # 2D 시각화 (코드 차원이 2면)\n",
    "    if cfg.layer_sizes[-1] == 2:\n",
    "        if cfg.dataset == \"mnist\":\n",
    "            plot_2d_codes(dae, Xtr, ytr, title=\"MNIST 2D codes (Autoencoder)\")\n",
    "        else:\n",
    "            plot_2d_codes(dae, Xtr, None, title=\"Curves 2D codes (Autoencoder)\")\n",
    "\n",
    "    return dae\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Entry\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) MNIST: 784-1000-500-250-30 (논문 표준)\n",
    "    cfg_mnist = ExpConfig(\n",
    "        dataset=\"mnist\",\n",
    "        layer_sizes=(784, 1000, 500, 250, 30),\n",
    "        rbm_epochs=5,   # 재현 높이려면 30+ 로 늘리기\n",
    "        ft_epochs=10,   # 30~100 추천\n",
    "        loss_type=\"bce\",\n",
    "        code_linear=True\n",
    "    )\n",
    "    dae_mnist = run_experiment(cfg_mnist)\n",
    "\n",
    "    # 2) Curves 합성: 784-400-200-100-50-25-6 (논문 예시)\n",
    "    # 곡선 생성에 scipy가 필요합니다: pip install scipy\n",
    "    cfg_curves = ExpConfig(\n",
    "        dataset=\"curves\",\n",
    "        layer_sizes=(784, 400, 200, 100, 50, 25, 6),\n",
    "        rbm_epochs=5,\n",
    "        ft_epochs=10,\n",
    "        loss_type=\"bce\",\n",
    "        code_linear=True\n",
    "    )\n",
    "    dae_curves = run_experiment(cfg_curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e444b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
