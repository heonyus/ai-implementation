{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336f66b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0+cu126'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0416bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cuda\n",
      "GPU 이름: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "print(f\"GPU 이름: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU만 사용 가능'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ad077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.23-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\projects\\ai-implementation\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------  11.8/12.0 MB 61.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 53.6 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 20.3 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 51.3 MB/s  0:00:00\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading regex-2025.10.23-cp312-cp312-win_amd64.whl (276 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/12 [urllib3]\n",
      "   ----------------------------------------  0/12 [urllib3]\n",
      "   --- ------------------------------------  1/12 [tqdm]\n",
      "   --- ------------------------------------  1/12 [tqdm]\n",
      "   --- ------------------------------------  1/12 [tqdm]\n",
      "   ---------- -----------------------------  3/12 [regex]\n",
      "   ------------- --------------------------  4/12 [pyyaml]\n",
      "   ---------------- -----------------------  5/12 [idna]\n",
      "   -------------------- -------------------  6/12 [charset_normalizer]\n",
      "   -------------------- -------------------  6/12 [charset_normalizer]\n",
      "   -------------------------- -------------  8/12 [requests]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [huggingface-hub]\n",
      "   --------------------------------- ------ 10/12 [tokenizers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ---------------------------------------- 12/12 [transformers]\n",
      "\n",
      "Successfully installed certifi-2025.10.5 charset_normalizer-3.4.4 huggingface-hub-0.36.0 idna-3.11 pyyaml-6.0.3 regex-2025.10.23 requests-2.32.5 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a616261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\ai-implementation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([1, 16]), 'token_type_ids': torch.Size([1, 16]), 'attention_mask': torch.Size([1, 16])}\n",
      "Output shape: torch.Size([1, 16, 256])\n",
      "첫 5개 토큰: ['[CLS]', '[CLS]', 'i', 'like', 'coffee']\n",
      "임베딩 첫 토큰 벡터 norm: 16.568382263183594\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 1. 설정\n",
    "cfg = {\n",
    "    \"vocab_size\": 30522,\n",
    "    \"max_len\": 64,\n",
    "    \"hidden_size\": 256,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# 2. 임베딩 정의\n",
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.emb_word = nn.Embedding(cfg[\"vocab_size\"], cfg[\"hidden_size\"])\n",
    "        self.emb_pos  = nn.Embedding(cfg[\"max_len\"], cfg[\"hidden_size\"])\n",
    "        self.emb_type = nn.Embedding(cfg[\"type_vocab_size\"], cfg[\"hidden_size\"])\n",
    "        self.ln = nn.LayerNorm(cfg[\"hidden_size\"])\n",
    "        self.do = nn.Dropout(cfg[\"dropout\"])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        pos = torch.arange(T, device=device).unsqueeze(0).expand(B, T)\n",
    "        w = self.emb_word(input_ids)\n",
    "        p = self.emb_pos(pos)\n",
    "        t = self.emb_type(token_type_ids)\n",
    "        x = self.ln(w + p + t)\n",
    "        x = self.do(x)\n",
    "        return x\n",
    "\n",
    "emb = BertEmbedding(cfg)\n",
    "\n",
    "# 3. 인터넷 예시 문장\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"[CLS] i like coffee [SEP] but my friend likes tea [SEP]\"\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16)\n",
    "print({k: v.shape for k, v in tokens.items()})  # input_ids, token_type_ids, attention_mask\n",
    "\n",
    "# 4. 임베딩 통과\n",
    "with torch.no_grad():\n",
    "    out = emb(tokens[\"input_ids\"], tokens[\"token_type_ids\"])\n",
    "print(\"Output shape:\", out.shape)  # (1, 16, 256)\n",
    "\n",
    "# 5. 확인\n",
    "print(\"첫 5개 토큰:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0][:5]))\n",
    "print(\"임베딩 첫 토큰 벡터 norm:\", out[0,0].norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aee6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 1. 설정\n",
    "cfg = {\n",
    "    \"vocab\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
